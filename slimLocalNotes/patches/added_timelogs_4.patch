diff --git llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
index ce2a96edb0..b543e5bb64 100644
--- llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
+++ llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
@@ -93,6 +93,8 @@ public void startThreads() {
   public DiskRangeList getFileData(Object fileKey, DiskRangeList ranges, long baseOffset,
       DiskRangeListFactory factory, LowLevelCacheCounters qfCounters, BooleanRef gotAllData) {
     if (ranges == null) return null;
+    long statTime = System.nanoTime();
+    //Add some measurements here. This is where the lookup in the cache happens
     DiskRangeList prev = ranges.prev;
     FileCache<ConcurrentSkipListMap<Long, LlapDataBuffer>> subCache = cache.get(fileKey);
     if (subCache == null || !subCache.incRef()) {
@@ -104,6 +106,7 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList ranges, long base
       if (prev != null && gotAllData != null) {
         gotAllData.value = false;
       }
+      LlapIoImpl.CACHE_LOGGER.info("Time to get file first exit {}", System.nanoTime() - statTime);
       return ranges;
     }
     try {
@@ -174,6 +177,7 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList ranges, long base
       LlapIoImpl.LOG.error(s);
       throw new RuntimeException(s);
     }
+    LlapIoImpl.CACHE_LOGGER.info("Time to get file second exit {}", System.nanoTime() - statTime);
     return prev.next;
   }
 
@@ -291,6 +295,7 @@ private boolean lockBuffer(LlapDataBuffer buffer, boolean doNotifyPolicy) {
   @Override
   public long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] buffers,
       long baseOffset, Priority priority, LowLevelCacheCounters qfCounters, String tag) {
+    long startTime = System.nanoTime();
     long[] result = null;
     assert buffers.length == ranges.length;
     FileCache<ConcurrentSkipListMap<Long, LlapDataBuffer>> subCache =
@@ -354,6 +359,8 @@ private boolean lockBuffer(LlapDataBuffer buffer, boolean doNotifyPolicy) {
     } finally {
       subCache.decRef();
     }
+
+    LlapIoImpl.CACHE_LOGGER.info("Put data into cache took {}", System.nanoTime() - startTime);
     return result;
   }
 
@@ -373,7 +380,9 @@ public void decRefBuffers(List<MemoryBuffer> cacheBuffers) {
     }
   }
 
+  //This where the buffers get unlocked
   private void unlockBuffer(LlapDataBuffer buffer, boolean handleLastDecRef) {
+    final long startTime = System.nanoTime();
     boolean isLastDecref = (buffer.decRef() == 0);
     if (handleLastDecRef && isLastDecref) {
       // This is kind of not pretty, but this is how we detect whether buffer was cached.
@@ -388,6 +397,7 @@ private void unlockBuffer(LlapDataBuffer buffer, boolean handleLastDecRef) {
       }
     }
     metrics.decrCacheNumLockedBuffers();
+    LlapIoImpl.CACHE_LOGGER.info("Time to unlock buffer {}", System.nanoTime() - startTime);
   }
 
   private static final ByteBuffer fakeBuf = ByteBuffer.wrap(new byte[1]);
diff --git llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
index 0a8ab876ba..a765011fb4 100644
--- llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
+++ llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
@@ -449,6 +449,7 @@ public void uncaughtException(final Thread t, final Throwable e) {
   ColumnVectorBatch nextCvb() throws InterruptedException, IOException {
     boolean isFirst = (lastCvb == null);
     if (!isFirst) {
+      //This guy return possibly unlock the old DS used form previous iteration
       feedback.returnData(lastCvb);
     }
 
diff --git llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java
index 780773d88a..9347466bbd 100644
--- llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java
+++ llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java
@@ -69,6 +69,7 @@ public MetadataCache(BuddyAllocator allocator, MemoryManager memoryManager,
 
   public void putIncompleteCbs(Object fileKey, DiskRange[] ranges, long baseOffset, AtomicBoolean isStopped) {
     if (estimateErrors == null) return;
+    long startTime = System.nanoTime();
     OrcFileEstimateErrors errorData = estimateErrors.get(fileKey);
     boolean isNew = false;
     // We should technically update memory usage if updating the old object, but we don't do it
@@ -98,6 +99,7 @@ public void putIncompleteCbs(Object fileKey, DiskRange[] ranges, long baseOffset
       }
     }
     policy.notifyUnlock(errorData);
+    LlapIoImpl.CACHE_LOGGER.info("Time to put incomplete metadata {}", System.nanoTime() - startTime);
   }
 
   public DiskRangeList getIncompleteCbs(
diff --git llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java
index 923042d88c..a5cde68d4a 100644
--- llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java
+++ llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java
@@ -182,7 +182,7 @@ public void testLruExtreme() {
   public void testPurge() {
     final int HEAP_SIZE = 32;
     Configuration conf = new Configuration();
-    conf.setFloat(HiveConf.ConfVars.LLAP_LRFU_LAMBDA.varname, 0.2f);
+    conf.setFloat(HiveConf.ConfVars.LLAP_LRFU_LAMBDA.varname, 1f);
     EvictionTracker et = new EvictionTracker();
     LowLevelLrfuCachePolicy lrfu = new LowLevelLrfuCachePolicy(1, HEAP_SIZE, conf);
     MetricsMock m = createMetricsMock();
diff --git ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
index 346ab5c8e7..e75ebaf5c1 100644
--- ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
+++ ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
@@ -297,6 +297,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
       OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,
       List<OrcProto.Stream> streamList, boolean[] physicalFileIncludes, boolean[] rgs,
       Consumer<OrcEncodedColumnBatch> consumer) throws IOException {
+    long startTime = System.nanoTime();
     // Note: for now we don't have to setError here, caller will setError if we throw.
     // We are also not supposed to call setDone, since we are only part of the operation.
     long stripeOffset = stripe.getOffset();
@@ -397,6 +398,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
       } else {
         LOG.warn("Nothing to read for stripe [" + stripe + "]");
       }
+      LOG.info("Read encoded time first exit {}", System.nanoTime() - startTime);
       return;
     }
 
@@ -571,6 +573,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
         releaseInitialRefcounts(toRead.next);
         // Release buffers as we are done with all the streams... also see toRelease comment.
         releaseBuffers(toRelease.keySet(), true);
+        LOG.info("Read encoded second exit {}", System.nanoTime() - startTime);
       } catch (Throwable t) {
         if (!hasError) throw new IOException(t);
         LOG.error("Error during the cleanup after another error; ignoring", t);
diff --git storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
index 29a3b0f2f4..db858abf73 100644
--- storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
+++ storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
@@ -103,6 +103,7 @@ public String toString() {
    */
   protected boolean[] hasData;
 
+  //This seems to be part of a hot loop that might need to be optimized
   public void reset() {
     if (hasData != null) {
       Arrays.fill(hasData, false);
@@ -143,6 +144,7 @@ public int getTotalColCount() {
     return columnData.length; // Includes the columns that have no data
   }
 
+  //This seems to be part of a hot loop that might need to be optimized
   protected void resetColumnArrays(int columnCount) {
     if (hasData != null && columnCount == hasData.length) {
       Arrays.fill(hasData, false);
