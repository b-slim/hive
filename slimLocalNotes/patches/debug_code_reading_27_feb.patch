diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java b/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
index 62d7e55344..3398688cbd 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
@@ -298,6 +298,11 @@ private boolean lockBuffer(LlapDataBuffer buffer, boolean doNotifyPolicy) {
     try {
       for (int i = 0; i < ranges.length; ++i) {
         LlapDataBuffer buffer = (LlapDataBuffer)buffers[i];
+        LlapIoImpl.LOG.info("PutFile file {}, buffer{}, length {} range offset {}",
+                        fileKey,
+                        buffers[i],
+                        ranges[i].getLength(),
+                        ranges[i].getOffset());
         if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {
           LlapIoImpl.LOCKING_LOGGER.trace("Locking {} at put time", buffer);
         }
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java b/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java
index 704f2f14d3..0381e06c26 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java
@@ -92,6 +92,10 @@ public void cache(LlapCacheableBuffer buffer, Priority priority) {
     long time = timer.incrementAndGet();
     buffer.priority = F0;
     buffer.lastUpdate = time;
+    LlapIoImpl.LOG.info("Asked-to-cache ->[{}], memory->[{}], tag->[{}]",
+                buffer.toStringForCache(),
+                buffer.getMemoryUsage(),
+                buffer.getTag());
     if (priority == Priority.HIGH) {
       // This is arbitrary. Note that metadata may come from a big scan and nuke all the data
       // from some small frequently accessed tables, because it gets such a large priority boost
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
index e6d8b7ab3e..8da8211c68 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
@@ -961,10 +961,10 @@ public DiskRangeList readFileData(DiskRangeList range, long baseOffset,
       long startTime = counters.startTimeCounter();
       DiskRangeList result = orcDataReaderRef.readFileData(range, baseOffset, doForceDirect);
       counters.recordHdfsTime(startTime);
-      if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
-        LlapIoImpl.ORC_LOGGER.trace("Disk ranges after disk read (file {}, base offset {}): {}",
+      //if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
+        LlapIoImpl.LOG.info("Disk ranges after disk read (file {}, base offset {}): {}",
             fileKey, baseOffset, RecordReaderUtils.stringifyDiskRanges(result));
-      }
+      //}
       trace.logRanges(fileKey, baseOffset, result, IoTrace.RangesSrc.DISK);
       return result;
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
index 346ab5c8e7..4a46300893 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
@@ -914,6 +914,7 @@ public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, lon
     DiskRange[] cacheKeys = new DiskRange[toDecompress.size()];
     int ix = 0;
     for (ProcCacheChunk chunk : toDecompress) {
+      // same question here not sure what means that we don't store this ?
       cacheKeys[ix] = chunk; // Relies on the fact that cache does not actually store these.
       targetBuffers[ix] = chunk.getBuffer();
       ++ix;
@@ -989,6 +990,8 @@ public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, lon
 
     // 6. Finally, put uncompressed data to cache.
     if (fileKey != null) {
+      // tracking this where the keys are already set with more buffers
+      // to make sure that we are adding it here add some logs about the cache keys here
       long[] collisionMask = cacheWrapper.putFileData(
           fileKey, cacheKeys, targetBuffers, baseOffset, tag);
       processCacheCollisions(collisionMask, toDecompress, targetBuffers, csd.getCacheBuffers());
