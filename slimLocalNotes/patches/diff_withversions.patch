diff --git a/bin/hive b/bin/hive
index e76eafcfc4..355282f2e3 100755
--- a/bin/hive
+++ b/bin/hive
@@ -358,6 +358,7 @@ fi
 # include the log4j jar that is used for hive into the classpath
 CLASSPATH="${CLASSPATH}:${LOG_JAR_CLASSPATH}"
 export HADOOP_CLASSPATH="${HADOOP_CLASSPATH}:${LOG_JAR_CLASSPATH}"
+export JVM_PID="$$"
 
 if [ "$TORUN" = "" ] ; then
   echo "Service $SERVICE not found"
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index f5ca25e740..d52a86a319 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -3985,6 +3985,7 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal
     LLAP_IO_VRB_QUEUE_LIMIT_MIN("hive.llap.io.vrb.queue.limit.min", 10,
         "The minimum queue size for VRBs produced by a LLAP IO thread when the processing is\n" +
         "slower than the IO (used when determining the size from base size)."),
+    //Ask @Prashant why this is off? what are the tests needed etc
     LLAP_IO_SHARE_OBJECT_POOLS("hive.llap.io.share.object.pools", false,
         "Whether to used shared object pools in LLAP IO. A safety flag."),
     LLAP_AUTO_ALLOW_UBER("hive.llap.auto.allow.uber", false,
diff --git a/common/src/java/org/apache/hive/http/HttpServer.java b/common/src/java/org/apache/hive/http/HttpServer.java
index 3cb7a33c4e..24c5422a18 100644
--- a/common/src/java/org/apache/hive/http/HttpServer.java
+++ b/common/src/java/org/apache/hive/http/HttpServer.java
@@ -21,6 +21,9 @@
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.net.URL;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.LinkedList;
@@ -558,6 +561,22 @@ private void initializeWebServer(final Builder b, int queueSize) throws IOExcept
     addServlet("conf", "/conf", ConfServlet.class);
     addServlet("stacks", "/stacks", StackServlet.class);
     addServlet("conflog", "/conflog", Log4j2ConfiguratorServlet.class);
+    final String asyncProfilerHome = ProfileServlet.getAsyncProfilerHome();
+    if (asyncProfilerHome != null && !asyncProfilerHome.trim().isEmpty()) {
+      addServlet("prof", "/prof", ProfileServlet.class);
+      Path tmpDir = Paths.get(ProfileServlet.OUTPUT_DIR);
+      if (Files.notExists(tmpDir)) {
+        Files.createDirectories(tmpDir);
+      }
+      ServletContextHandler genCtx =
+        new ServletContextHandler(contexts, "/prof-output");
+      setContextAttributes(genCtx.getServletContext(), b.contextAttrs);
+      genCtx.addServlet(ProfileOutputServlet.class, "/*");
+      genCtx.setResourceBase(tmpDir.toAbsolutePath().toString());
+      genCtx.setDisplayName("prof-output");
+    } else {
+      LOG.info("ASYNC_PROFILER_HOME env or -Dasync.profiler.home not specified. Disabling /prof endpoint..");
+    }
 
     for (Pair<String, Class<? extends HttpServlet>> p : b.servlets) {
       addServlet(p.getFirst(), "/" + p.getFirst(), p.getSecond());
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java b/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
index ce2a96edb0..b543e5bb64 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
@@ -93,6 +93,8 @@ public void startThreads() {
   public DiskRangeList getFileData(Object fileKey, DiskRangeList ranges, long baseOffset,
       DiskRangeListFactory factory, LowLevelCacheCounters qfCounters, BooleanRef gotAllData) {
     if (ranges == null) return null;
+    long statTime = System.nanoTime();
+    //Add some measurements here. This is where the lookup in the cache happens
     DiskRangeList prev = ranges.prev;
     FileCache<ConcurrentSkipListMap<Long, LlapDataBuffer>> subCache = cache.get(fileKey);
     if (subCache == null || !subCache.incRef()) {
@@ -104,6 +106,7 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList ranges, long base
       if (prev != null && gotAllData != null) {
         gotAllData.value = false;
       }
+      LlapIoImpl.CACHE_LOGGER.info("Time to get file first exit {}", System.nanoTime() - statTime);
       return ranges;
     }
     try {
@@ -174,6 +177,7 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList ranges, long base
       LlapIoImpl.LOG.error(s);
       throw new RuntimeException(s);
     }
+    LlapIoImpl.CACHE_LOGGER.info("Time to get file second exit {}", System.nanoTime() - statTime);
     return prev.next;
   }
 
@@ -291,6 +295,7 @@ private boolean lockBuffer(LlapDataBuffer buffer, boolean doNotifyPolicy) {
   @Override
   public long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] buffers,
       long baseOffset, Priority priority, LowLevelCacheCounters qfCounters, String tag) {
+    long startTime = System.nanoTime();
     long[] result = null;
     assert buffers.length == ranges.length;
     FileCache<ConcurrentSkipListMap<Long, LlapDataBuffer>> subCache =
@@ -354,6 +359,8 @@ private boolean lockBuffer(LlapDataBuffer buffer, boolean doNotifyPolicy) {
     } finally {
       subCache.decRef();
     }
+
+    LlapIoImpl.CACHE_LOGGER.info("Put data into cache took {}", System.nanoTime() - startTime);
     return result;
   }
 
@@ -373,7 +380,9 @@ public void decRefBuffers(List<MemoryBuffer> cacheBuffers) {
     }
   }
 
+  //This where the buffers get unlocked
   private void unlockBuffer(LlapDataBuffer buffer, boolean handleLastDecRef) {
+    final long startTime = System.nanoTime();
     boolean isLastDecref = (buffer.decRef() == 0);
     if (handleLastDecRef && isLastDecref) {
       // This is kind of not pretty, but this is how we detect whether buffer was cached.
@@ -388,6 +397,7 @@ private void unlockBuffer(LlapDataBuffer buffer, boolean handleLastDecRef) {
       }
     }
     metrics.decrCacheNumLockedBuffers();
+    LlapIoImpl.CACHE_LOGGER.info("Time to unlock buffer {}", System.nanoTime() - startTime);
   }
 
   private static final ByteBuffer fakeBuf = ByteBuffer.wrap(new byte[1]);
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
index 0a8ab876ba..a765011fb4 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
@@ -449,6 +449,7 @@ public void uncaughtException(final Thread t, final Throwable e) {
   ColumnVectorBatch nextCvb() throws InterruptedException, IOException {
     boolean isFirst = (lastCvb == null);
     if (!isFirst) {
+      //This guy return possibly unlock the old DS used form previous iteration
       feedback.returnData(lastCvb);
     }
 
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
index 85fbe6eaa0..af1ebf14e6 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
@@ -107,7 +107,7 @@
  * it inserts itself into the pipeline to put the data in cache, before passing it to the real
  * consumer. It also serves as ConsumerFeedback that receives processed EncodedColumnBatch-es.
  */
-public class OrcEncodedDataReader extends CallableWithNdc<Void>
+@SuppressWarnings("Duplicates") public class OrcEncodedDataReader extends CallableWithNdc<Void>
     implements ConsumerFeedback<OrcEncodedColumnBatch>, TezCounterSource {
   private static final Logger LOG = LoggerFactory.getLogger(OrcEncodedDataReader.class);
   public static final FixedSizedObjectPool<ColumnStreamData> CSD_POOL =
@@ -217,9 +217,9 @@ public OrcEncodedDataReader(LowLevelCache lowLevelCache, BufferUsageManager buff
         HiveConf.getBoolVar(daemonConf, ConfVars.LLAP_CACHE_DEFAULT_FS_FILE_ID),
         !HiveConf.getBoolVar(daemonConf, ConfVars.LLAP_IO_USE_FILEID_PATH)
         );
+    //This has a call to HDFS maybe need to report the time?
     fileMetadata = getFileFooterFromCacheOrDisk();
     final TypeDescription fileSchema = fileMetadata.getSchema();
-
     fileIncludes = includes.generateFileIncludes(fileSchema);
     if (LOG.isDebugEnabled()) {
       LOG.debug("From {}, the file includes are {}", includes, DebugUtils.toString(fileIncludes));
@@ -277,8 +277,7 @@ protected Void performDataRead() throws IOException, InterruptedException {
     }
     counters.setDesc(QueryFragmentCounters.Desc.TABLE,
         LlapUtil.getDbAndTableNameForMetrics(split.getPath(), false));
-    counters.setDesc(QueryFragmentCounters.Desc.FILE, split.getPath()
-        + (fileKey == null ? "" : " (" + fileKey + ")"));
+    counters.setDesc(QueryFragmentCounters.Desc.FILE, split.getPath() + (fileKey == null ? "" : " (" + fileKey + ")"));
     try {
       validateFileMetadata();
 
@@ -751,6 +750,7 @@ public void returnData(OrcEncodedColumnBatch ecb) {
       ColumnStreamData[] datas = ecb.getColumnData(colIx);
       for (ColumnStreamData data : datas) {
         if (data == null || data.decRef() != 0) continue;
+        // This can be hotloop locking part
         if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {
           for (MemoryBuffer buf : data.getCacheBuffers()) {
             LlapIoImpl.LOCKING_LOGGER.trace("Unlocking {} at the end of processing", buf);
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java
index 780773d88a..9347466bbd 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java
@@ -69,6 +69,7 @@ public MetadataCache(BuddyAllocator allocator, MemoryManager memoryManager,
 
   public void putIncompleteCbs(Object fileKey, DiskRange[] ranges, long baseOffset, AtomicBoolean isStopped) {
     if (estimateErrors == null) return;
+    long startTime = System.nanoTime();
     OrcFileEstimateErrors errorData = estimateErrors.get(fileKey);
     boolean isNew = false;
     // We should technically update memory usage if updating the old object, but we don't do it
@@ -98,6 +99,7 @@ public void putIncompleteCbs(Object fileKey, DiskRange[] ranges, long baseOffset
       }
     }
     policy.notifyUnlock(errorData);
+    LlapIoImpl.CACHE_LOGGER.info("Time to put incomplete metadata {}", System.nanoTime() - startTime);
   }
 
   public DiskRangeList getIncompleteCbs(
diff --git a/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java b/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java
index 923042d88c..a5cde68d4a 100644
--- a/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java
+++ b/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java
@@ -182,7 +182,7 @@ public void testLruExtreme() {
   public void testPurge() {
     final int HEAP_SIZE = 32;
     Configuration conf = new Configuration();
-    conf.setFloat(HiveConf.ConfVars.LLAP_LRFU_LAMBDA.varname, 0.2f);
+    conf.setFloat(HiveConf.ConfVars.LLAP_LRFU_LAMBDA.varname, 1f);
     EvictionTracker et = new EvictionTracker();
     LowLevelLrfuCachePolicy lrfu = new LowLevelLrfuCachePolicy(1, HEAP_SIZE, conf);
     MetricsMock m = createMetricsMock();
diff --git a/pom.xml b/pom.xml
index 64cd3f7db9..fd0e99cc38 100644
--- a/pom.xml
+++ b/pom.xml
@@ -127,7 +127,7 @@
     <avatica.version>1.10.0.3.1.0.0-SNAPSHOT</avatica.version>
     <avro.version>1.7.7</avro.version>
     <bonecp.version>0.8.0.RELEASE</bonecp.version>
-    <calcite.version>1.16.0.3.1.0.0-SNAPSHOT</calcite.version>
+    <calcite.version>1.16.0.3.1.1.0-36</calcite.version>
     <datanucleus-api-jdo.version>4.2.4</datanucleus-api-jdo.version>
     <datanucleus-core.version>4.1.17</datanucleus-core.version>
     <datanucleus-rdbms.version>4.1.19</datanucleus-rdbms.version>
@@ -150,7 +150,7 @@
     <guava.version>19.0</guava.version>
     <groovy.version>2.4.11</groovy.version>
     <h2database.version>1.3.166</h2database.version>
-    <hadoop.version>3.1.1.3.1.0.0-SNAPSHOT</hadoop.version>
+    <hadoop.version>3.1.1.3.1.1.0-36</hadoop.version>
     <hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop</hadoop.bin.path>
     <hamcrest.version>1.3</hamcrest.version>
     <hbase.version>2.0.2.3.1.0.0-SNAPSHOT</hbase.version>
@@ -186,7 +186,7 @@
     <libthrift.version>0.9.3</libthrift.version>
     <log4j2.version>2.10.0</log4j2.version>
     <opencsv.version>2.3</opencsv.version>
-    <orc.version>1.5.2-SNAPSHOT</orc.version>
+    <orc.version>1.5.1.3.1.1.0-33</orc.version>
     <mockito-all.version>1.10.19</mockito-all.version>
     <powermock.version>1.7.4</powermock.version>
     <mina.version>2.0.0-M5</mina.version>
@@ -199,8 +199,8 @@
     <stax.version>1.0.1</stax.version>
     <slf4j.version>1.7.10</slf4j.version>
     <ST4.version>4.0.4</ST4.version>
-    <storage-api.version>2.6.1-SNAPSHOT</storage-api.version>
-    <tez.version>0.10.0-SNAPSHOT</tez.version>
+    <storage-api.version>2.3.0.3.1.1.0-36</storage-api.version>
+    <tez.version>0.9.1.3.1.1.0-36</tez.version>
     <super-csv.version>2.2.0</super-csv.version>
     <spark.version>2.3.0</spark.version>
     <scala.binary.version>2.11</scala.binary.version>
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
index 346ab5c8e7..e75ebaf5c1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
@@ -297,6 +297,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
       OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,
       List<OrcProto.Stream> streamList, boolean[] physicalFileIncludes, boolean[] rgs,
       Consumer<OrcEncodedColumnBatch> consumer) throws IOException {
+    long startTime = System.nanoTime();
     // Note: for now we don't have to setError here, caller will setError if we throw.
     // We are also not supposed to call setDone, since we are only part of the operation.
     long stripeOffset = stripe.getOffset();
@@ -397,6 +398,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
       } else {
         LOG.warn("Nothing to read for stripe [" + stripe + "]");
       }
+      LOG.info("Read encoded time first exit {}", System.nanoTime() - startTime);
       return;
     }
 
@@ -571,6 +573,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
         releaseInitialRefcounts(toRead.next);
         // Release buffers as we are done with all the streams... also see toRelease comment.
         releaseBuffers(toRelease.keySet(), true);
+        LOG.info("Read encoded second exit {}", System.nanoTime() - startTime);
       } catch (Throwable t) {
         if (!hasError) throw new IOException(t);
         LOG.error("Error during the cleanup after another error; ignoring", t);
diff --git a/shims/common/pom.xml b/shims/common/pom.xml
index 0b7b6768e4..f4af85e1b1 100644
--- a/shims/common/pom.xml
+++ b/shims/common/pom.xml
@@ -91,6 +91,22 @@
       <artifactId>junit</artifactId>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-core</artifactId>
+      <version>3.1.1.3.1.1.0-33</version>
+      <scope>compile</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-common</artifactId>
+      <version>3.1.1.3.1.1.0-33</version>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-core</artifactId>
+      <version>3.1.1.3.1.1.0-33</version>
+    </dependency>
   </dependencies>
 
   <build>
diff --git a/storage-api/pom.xml b/storage-api/pom.xml
index a40feff575..751d43aeaf 100644
--- a/storage-api/pom.xml
+++ b/storage-api/pom.xml
@@ -25,7 +25,7 @@
 
   <groupId>org.apache.hive</groupId>
   <artifactId>hive-storage-api</artifactId>
-  <version>2.6.1-SNAPSHOT</version>
+  <version>2.3.0.3.1.1.0-36</version>
   <packaging>jar</packaging>
   <name>Hive Storage API</name>
 
@@ -33,7 +33,7 @@
     <commons-lang.version>2.6</commons-lang.version>
     <commons-logging.version>1.1.3</commons-logging.version>
     <guava.version>19.0</guava.version>
-    <hadoop.version>3.0.0-beta1</hadoop.version>
+    <hadoop.version>3.1.1.3.1.1.0-36</hadoop.version>
     <junit.version>4.11</junit.version>
     <slf4j.version>1.7.10</slf4j.version>
     <maven.checkstyle.plugin.version>2.17</maven.checkstyle.plugin.version>
diff --git a/storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java b/storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
index 29a3b0f2f4..db858abf73 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
@@ -103,6 +103,7 @@ public String toString() {
    */
   protected boolean[] hasData;
 
+  //This seems to be part of a hot loop that might need to be optimized
   public void reset() {
     if (hasData != null) {
       Arrays.fill(hasData, false);
@@ -143,6 +144,7 @@ public int getTotalColCount() {
     return columnData.length; // Includes the columns that have no data
   }
 
+  //This seems to be part of a hot loop that might need to be optimized
   protected void resetColumnArrays(int columnCount) {
     if (hasData != null && columnCount == hasData.length) {
       Arrays.fill(hasData, false);
