diff --git bin/hive bin/hive
index e76eafcfc4..355282f2e3 100755
--- bin/hive
+++ bin/hive
@@ -358,6 +358,7 @@ fi
 # include the log4j jar that is used for hive into the classpath
 CLASSPATH="${CLASSPATH}:${LOG_JAR_CLASSPATH}"
 export HADOOP_CLASSPATH="${HADOOP_CLASSPATH}:${LOG_JAR_CLASSPATH}"
+export JVM_PID="$$"
 
 if [ "$TORUN" = "" ] ; then
   echo "Service $SERVICE not found"
diff --git common/src/java/org/apache/hadoop/hive/conf/HiveConf.java common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index f5ca25e740..d52a86a319 100644
--- common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -3985,6 +3985,7 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal
     LLAP_IO_VRB_QUEUE_LIMIT_MIN("hive.llap.io.vrb.queue.limit.min", 10,
         "The minimum queue size for VRBs produced by a LLAP IO thread when the processing is\n" +
         "slower than the IO (used when determining the size from base size)."),
+    //Ask @Prashant why this is off? what are the tests needed etc
     LLAP_IO_SHARE_OBJECT_POOLS("hive.llap.io.share.object.pools", false,
         "Whether to used shared object pools in LLAP IO. A safety flag."),
     LLAP_AUTO_ALLOW_UBER("hive.llap.auto.allow.uber", false,
diff --git llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
index ce2a96edb0..b543e5bb64 100644
--- llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
+++ llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
@@ -93,6 +93,8 @@ public void startThreads() {
   public DiskRangeList getFileData(Object fileKey, DiskRangeList ranges, long baseOffset,
       DiskRangeListFactory factory, LowLevelCacheCounters qfCounters, BooleanRef gotAllData) {
     if (ranges == null) return null;
+    long statTime = System.nanoTime();
+    //Add some measurements here. This is where the lookup in the cache happens
     DiskRangeList prev = ranges.prev;
     FileCache<ConcurrentSkipListMap<Long, LlapDataBuffer>> subCache = cache.get(fileKey);
     if (subCache == null || !subCache.incRef()) {
@@ -104,6 +106,7 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList ranges, long base
       if (prev != null && gotAllData != null) {
         gotAllData.value = false;
       }
+      LlapIoImpl.CACHE_LOGGER.info("Time to get file first exit {}", System.nanoTime() - statTime);
       return ranges;
     }
     try {
@@ -174,6 +177,7 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList ranges, long base
       LlapIoImpl.LOG.error(s);
       throw new RuntimeException(s);
     }
+    LlapIoImpl.CACHE_LOGGER.info("Time to get file second exit {}", System.nanoTime() - statTime);
     return prev.next;
   }
 
@@ -291,6 +295,7 @@ private boolean lockBuffer(LlapDataBuffer buffer, boolean doNotifyPolicy) {
   @Override
   public long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] buffers,
       long baseOffset, Priority priority, LowLevelCacheCounters qfCounters, String tag) {
+    long startTime = System.nanoTime();
     long[] result = null;
     assert buffers.length == ranges.length;
     FileCache<ConcurrentSkipListMap<Long, LlapDataBuffer>> subCache =
@@ -354,6 +359,8 @@ private boolean lockBuffer(LlapDataBuffer buffer, boolean doNotifyPolicy) {
     } finally {
       subCache.decRef();
     }
+
+    LlapIoImpl.CACHE_LOGGER.info("Put data into cache took {}", System.nanoTime() - startTime);
     return result;
   }
 
@@ -373,7 +380,9 @@ public void decRefBuffers(List<MemoryBuffer> cacheBuffers) {
     }
   }
 
+  //This where the buffers get unlocked
   private void unlockBuffer(LlapDataBuffer buffer, boolean handleLastDecRef) {
+    final long startTime = System.nanoTime();
     boolean isLastDecref = (buffer.decRef() == 0);
     if (handleLastDecRef && isLastDecref) {
       // This is kind of not pretty, but this is how we detect whether buffer was cached.
@@ -388,6 +397,7 @@ private void unlockBuffer(LlapDataBuffer buffer, boolean handleLastDecRef) {
       }
     }
     metrics.decrCacheNumLockedBuffers();
+    LlapIoImpl.CACHE_LOGGER.info("Time to unlock buffer {}", System.nanoTime() - startTime);
   }
 
   private static final ByteBuffer fakeBuf = ByteBuffer.wrap(new byte[1]);
diff --git llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
index 0a8ab876ba..a765011fb4 100644
--- llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
+++ llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
@@ -449,6 +449,7 @@ public void uncaughtException(final Thread t, final Throwable e) {
   ColumnVectorBatch nextCvb() throws InterruptedException, IOException {
     boolean isFirst = (lastCvb == null);
     if (!isFirst) {
+      //This guy return possibly unlock the old DS used form previous iteration
       feedback.returnData(lastCvb);
     }
 
diff --git llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
index 85fbe6eaa0..035a211efc 100644
--- llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
+++ llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
@@ -219,7 +219,6 @@ public OrcEncodedDataReader(LowLevelCache lowLevelCache, BufferUsageManager buff
         );
     fileMetadata = getFileFooterFromCacheOrDisk();
     final TypeDescription fileSchema = fileMetadata.getSchema();
-
     fileIncludes = includes.generateFileIncludes(fileSchema);
     if (LOG.isDebugEnabled()) {
       LOG.debug("From {}, the file includes are {}", includes, DebugUtils.toString(fileIncludes));
@@ -751,6 +750,7 @@ public void returnData(OrcEncodedColumnBatch ecb) {
       ColumnStreamData[] datas = ecb.getColumnData(colIx);
       for (ColumnStreamData data : datas) {
         if (data == null || data.decRef() != 0) continue;
+        // This can be hotloop locking part
         if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {
           for (MemoryBuffer buf : data.getCacheBuffers()) {
             LlapIoImpl.LOCKING_LOGGER.trace("Unlocking {} at the end of processing", buf);
diff --git llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java
index 780773d88a..9347466bbd 100644
--- llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java
+++ llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java
@@ -69,6 +69,7 @@ public MetadataCache(BuddyAllocator allocator, MemoryManager memoryManager,
 
   public void putIncompleteCbs(Object fileKey, DiskRange[] ranges, long baseOffset, AtomicBoolean isStopped) {
     if (estimateErrors == null) return;
+    long startTime = System.nanoTime();
     OrcFileEstimateErrors errorData = estimateErrors.get(fileKey);
     boolean isNew = false;
     // We should technically update memory usage if updating the old object, but we don't do it
@@ -98,6 +99,7 @@ public void putIncompleteCbs(Object fileKey, DiskRange[] ranges, long baseOffset
       }
     }
     policy.notifyUnlock(errorData);
+    LlapIoImpl.CACHE_LOGGER.info("Time to put incomplete metadata {}", System.nanoTime() - startTime);
   }
 
   public DiskRangeList getIncompleteCbs(
diff --git llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java
index 923042d88c..a5cde68d4a 100644
--- llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java
+++ llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java
@@ -182,7 +182,7 @@ public void testLruExtreme() {
   public void testPurge() {
     final int HEAP_SIZE = 32;
     Configuration conf = new Configuration();
-    conf.setFloat(HiveConf.ConfVars.LLAP_LRFU_LAMBDA.varname, 0.2f);
+    conf.setFloat(HiveConf.ConfVars.LLAP_LRFU_LAMBDA.varname, 1f);
     EvictionTracker et = new EvictionTracker();
     LowLevelLrfuCachePolicy lrfu = new LowLevelLrfuCachePolicy(1, HEAP_SIZE, conf);
     MetricsMock m = createMetricsMock();
diff --git ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
index 346ab5c8e7..e75ebaf5c1 100644
--- ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
+++ ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
@@ -297,6 +297,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
       OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,
       List<OrcProto.Stream> streamList, boolean[] physicalFileIncludes, boolean[] rgs,
       Consumer<OrcEncodedColumnBatch> consumer) throws IOException {
+    long startTime = System.nanoTime();
     // Note: for now we don't have to setError here, caller will setError if we throw.
     // We are also not supposed to call setDone, since we are only part of the operation.
     long stripeOffset = stripe.getOffset();
@@ -397,6 +398,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
       } else {
         LOG.warn("Nothing to read for stripe [" + stripe + "]");
       }
+      LOG.info("Read encoded time first exit {}", System.nanoTime() - startTime);
       return;
     }
 
@@ -571,6 +573,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
         releaseInitialRefcounts(toRead.next);
         // Release buffers as we are done with all the streams... also see toRelease comment.
         releaseBuffers(toRelease.keySet(), true);
+        LOG.info("Read encoded second exit {}", System.nanoTime() - startTime);
       } catch (Throwable t) {
         if (!hasError) throw new IOException(t);
         LOG.error("Error during the cleanup after another error; ignoring", t);
diff --git storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
index 29a3b0f2f4..db858abf73 100644
--- storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
+++ storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
@@ -103,6 +103,7 @@ public String toString() {
    */
   protected boolean[] hasData;
 
+  //This seems to be part of a hot loop that might need to be optimized
   public void reset() {
     if (hasData != null) {
       Arrays.fill(hasData, false);
@@ -143,6 +144,7 @@ public int getTotalColCount() {
     return columnData.length; // Includes the columns that have no data
   }
 
+  //This seems to be part of a hot loop that might need to be optimized
   protected void resetColumnArrays(int columnCount) {
     if (hasData != null && columnCount == hasData.length) {
       Arrays.fill(hasData, false);
